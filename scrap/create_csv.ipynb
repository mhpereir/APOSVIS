{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "645daed1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py as h5py\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "datapath   = '../data/V1_MR_fix'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "76a19341",
   "metadata": {},
   "outputs": [],
   "source": [
    "def snapshot_redshifts(datapath):\n",
    "    file_list_temp = os.listdir(datapath)\n",
    "    snap_file_list = [file_.split('particledata_')[1] for file_ in file_list_temp if 'particledata' in file_]\n",
    "    snap_id_list   = [int(file_.split('_')[0]) for file_ in snap_file_list]\n",
    "\n",
    "    temp = sorted(zip(snap_id_list, snap_file_list))\n",
    "\n",
    "    snap_id_list   = [t[0] for t in temp]\n",
    "    snap_file_list = [t[1] for t in temp]\n",
    "\n",
    "    redshift_list  = [file_.split('_')[-1] for file_ in snap_file_list]\n",
    "    redshift_list  = [int(zzz[1:4].split('p')[0]) + int(zzz[5:])/1000 for zzz in redshift_list]\n",
    "    redshift_list  = np.asarray(redshift_list)\n",
    "    \n",
    "    return [snap_file_list, snap_id_list, redshift_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "591ef559",
   "metadata": {},
   "outputs": [],
   "source": [
    "snap_labels = snapshot_redshifts(datapath)\n",
    "snap_label_list = snap_labels[0]\n",
    "snap_num_list   = snap_labels[1]\n",
    "snap_red_list   = snap_labels[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f5de4179",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['127_z000p000']\n"
     ]
    }
   ],
   "source": [
    "print(snap_label_list)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d869ff33",
   "metadata": {},
   "source": [
    "# Create values for GROUP TABLE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f84d73d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_group_file(gpath, gfile):\n",
    "    dict_out = {}\n",
    "        \n",
    "    fofs                = []      #group ID\n",
    "    subs                = []      #subgroup ID\n",
    "    cops                = []      #center of potential\n",
    "    Vmax                = []      #max vel\n",
    "    Mdm                 = []      #mass dark matter\n",
    "    Ms                  = []      #mass stars\n",
    "    Mgas                = []      #mass gas\n",
    "    Rmax                = []      #max R?\n",
    "    \n",
    "    FOFCOP              = []\n",
    "    R200                = []\n",
    "    M200                = []\n",
    "    FirstSubID          = []  \n",
    "    Contamination_count = []      #number of subhalos that are lower res\n",
    "                                #we won't need this as we will avoid contamination particles by selecting subhalos inside a 2Mpc sphere.\n",
    "\n",
    "\n",
    "\n",
    "    with h5py.File(gpath+gfile+'.0.hdf5', 'r') as f:\n",
    "        a = f['Header'].attrs['Time']                       #scale factor\n",
    "        h = f['Header'].attrs['HubbleParam'] \n",
    "            \n",
    "    n=0\n",
    "    while os.path.exists(gpath+gfile+'.'+str(n)+'.hdf5'):\n",
    "        with h5py.File(gpath+gfile+'.'+str(n)+'.hdf5', 'r') as f:\n",
    "            try:\n",
    "                fofs.extend(f['Subhalo/GroupNumber'][()])\n",
    "                subs.extend(f['Subhalo/SubGroupNumber'][()])\n",
    "                cops.extend(f['Subhalo/CentreOfPotential'][()]*a/h)   \n",
    "                Vmax.extend(f['Subhalo/Vmax'][()])\n",
    "                Rmax.extend(f['Subhalo/VmaxRadius'][()])   \n",
    "                Mgas.extend(f['Subhalo/MassType'][:,0]*1e10/h)\n",
    "                Mdm.extend(f['Subhalo/MassType'][:,1]*1e10/h)\n",
    "                Ms.extend(f['Subhalo/MassType'][:,4]*1e10/h)\n",
    "\n",
    "                FOFCOP.extend(f['FOF/GroupCentreOfPotential']*a/h)\n",
    "                R200.extend(f['FOF/Group_R_Crit200'][()]*a/h)\n",
    "                M200.extend(f['FOF/Group_M_Crit200'][()]*1e10/h)\n",
    "                FirstSubID.extend(f['FOF/FirstSubhaloID'][()])\n",
    "                Contamination_count.extend( f['FOF/ContaminationCount'][()])\n",
    "            except:\n",
    "                pass\n",
    "            n+=1\n",
    "\n",
    "\n",
    "\n",
    "    print('Sucessfully read {} files.'.format(n))\n",
    "\n",
    "    # It  is very useful to work with \"numpy arrays\" (mathematical calculations are allowed). So I transform the python lists to  numpy arrays. (This I will do all the time as you will see below)\n",
    "    # these arrays have length = to the number of subhalos\n",
    "    fofs = np.array(fofs)   \n",
    "    subs = np.array(subs)\n",
    "    cops = np.array(cops)\n",
    "    Vmax = np.array(Vmax)\n",
    "    Mdm  = np.array(Mdm)\n",
    "    Ms   = np.array(Ms)\n",
    "    Mgas = np.array(Mgas)\n",
    "    Rmax = np.array(Rmax)  \n",
    "\n",
    "    subs_indxs = np.arange(0,len(subs))\n",
    "\n",
    "    # these arrays have length = to the number of fofs\n",
    "    FOFCOP     = np.array(FOFCOP)\n",
    "    R200       = np.array(R200)\n",
    "    M200       = np.array(M200)\n",
    "    FirstSubID = np.array(FirstSubID)\n",
    "    Contamination_count= np.array(Contamination_count)\n",
    "\n",
    "\n",
    "    #fig,ax = plt.subplots()\n",
    "    #ax.loglog(Mgas, Ms, 'ko')\n",
    "    #ax.set_xlabel('Mgas')\n",
    "    #ax.set_ylabel('Mstar')\n",
    "\n",
    "    ### In principle, we only want to keep subhalos with virial masses M200>1e10 Msun (less is poorer resolution)\n",
    "    fofIDs= np.where( (Contamination_count==0) )   # (no contamination particles)                      ### WARNING\n",
    "    #Since FOF groups are ordered by mass, starting at fofID=1, this numpy.where array is giving me (FofID-1). \n",
    "\n",
    "    FOFCOP = FOFCOP[fofIDs]\n",
    "    M200   = M200[fofIDs] \n",
    "    R200   = R200[fofIDs]\n",
    "\n",
    "    \n",
    "\n",
    "    # ## In addition,  We only will keep the \"centrals\" ,i.e.,  subs==0\n",
    "    # ctr_1        = cops[(fofs<=20)*(subs==0)]\n",
    "    # mdm_1        = Mdm [(fofs<=20)*(subs==0)]\n",
    "    # #ctr_1_w      = np.asarray([mdm_1[i] * ctr_1[i,:] for i in range(0,20)])\n",
    "    # #ctr_midpoint = np.mean(ctr_1_w, axis=0)/np.sum(mdm_1)\n",
    "\n",
    "    # if len(ctr_1) == 0:\n",
    "    #     pass\n",
    "\n",
    "    # else:\n",
    "\n",
    "    #     ctr_midpoint = np.average(ctr_1, axis=0, weights=mdm_1)\n",
    "\n",
    "    #     #print(ctr_1_w)\n",
    "    #     print(ctr_midpoint)\n",
    "\n",
    "    #     D    = np.sqrt((ctr_midpoint[0]-cops[:,0])**2 + (ctr_midpoint[1]-cops[:,1])**2 + (ctr_midpoint[2]-cops[:,2])**2)\n",
    "\n",
    "    cent = np.where((subs==0))\n",
    "\n",
    "    fofs = fofs[cent]\n",
    "    subs = subs[cent]\n",
    "    cops = cops[cent]\n",
    "    Vmax = Vmax[cent]\n",
    "    Mdm  = Mdm[cent]\n",
    "    Ms   = Ms[cent]\n",
    "    Mgas = Mgas[cent]\n",
    "    Rmax = Rmax[cent]\n",
    "\n",
    "    subs_indxs = subs_indxs[cent]\n",
    "\n",
    "    # the real fofID number is the numpy.where array above +1\n",
    "    fofIDs_real = fofIDs[0] + 1\n",
    "    which       = np.isin( fofs, fofIDs_real) \n",
    "    #It filters out the fofs we are interested in  (i.e., fofIDs_real) from the total fofs list (i.e., fofs).\n",
    "\n",
    "    fofs = fofs[which]\n",
    "    subs = subs[which]\n",
    "    cops = cops[which]\n",
    "    Vmax = Vmax[which]\n",
    "    Mdm  = Mdm[which]\n",
    "    Ms   = Ms[which]\n",
    "    Mgas = Mgas[which]\n",
    "    Rmax = Rmax[which]\n",
    "\n",
    "    subs_indxs = subs_indxs[which]\n",
    "\n",
    "    which = np.isin(fofIDs_real, fofs)\n",
    "    M200  = M200[which]\n",
    "    R200  = R200[which]\n",
    "\n",
    "    dict_out['fofs']       = fofs\n",
    "    dict_out['subs']       = subs\n",
    "    dict_out['cops']       = cops\n",
    "    # dict_out['Vmax']       = Vmax\n",
    "    # dict_out['Mdm']        = Mdm\n",
    "    # dict_out['Ms']         = Ms\n",
    "    # dict_out['Mgas']       = Mgas\n",
    "    dict_out['M200']       = M200\n",
    "    dict_out['R200']       = R200\n",
    "    # dict_out['subs_indxs'] = subs_indxs\n",
    "\n",
    "    # dict_out['ctr_midp']   = ctr_midpoint\n",
    "\n",
    "    return dict_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b9e30019",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sucessfully read 192 files.\n",
      "(10599, 7)\n"
     ]
    }
   ],
   "source": [
    "output_arr = np.empty((0, 7))\n",
    "for snap_label, snap_num in zip(snap_label_list, snap_num_list):\n",
    "\n",
    "  datasuffix = snap_label\n",
    "  gpath = datapath+'/groups_'+datasuffix+'/'\n",
    "  gfile = 'eagle_subfind_tab_'+datasuffix\n",
    "\n",
    "  dict_out = read_group_file(gpath, gfile)\n",
    "\n",
    "  snap_arr = np.zeros(len(dict_out['fofs'])) + snap_num\n",
    "\n",
    "  output_arr_temp = np.concatenate( (dict_out['fofs'].reshape(-1,1),\n",
    "                                snap_arr.reshape(-1,1),\n",
    "                                dict_out['M200'].reshape(-1,1),\n",
    "                                  dict_out['R200'].reshape(-1,1),\n",
    "                                    dict_out['cops'].reshape(-1,3)), axis=1)\n",
    "  print(output_arr_temp.shape)\n",
    "  output_arr = np.concatenate((output_arr, output_arr_temp), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "282ea41b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[127. 127. 127. ... 127. 127. 127.]\n"
     ]
    }
   ],
   "source": [
    "print(output_arr[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a178b3d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(data=output_arr, columns=['fofs', 'snpnr', 'M200', 'R200', 'cops_x', 'cops_y', 'cops_z'])#, dtype=[int, float, float, float, float, float])\n",
    "df['fofs'] = df['fofs'].astype('int')\n",
    "df['snpnr'] = df['snpnr'].astype('int')\n",
    "df.to_csv(path_or_buf='GROUP_TABLE_MR.csv', sep=',', index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "441cf553",
   "metadata": {},
   "source": [
    "# Create values for SUBGROUP TABLE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "11d307fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_subgroup_file(gpath, gfile):\n",
    "    dict_out = {}\n",
    "\n",
    "    fofs                = []      #group ID\n",
    "    subs                = []      #subgroup ID\n",
    "    cops                = []      #center of potential\n",
    "    Vmax                = []      #max vel\n",
    "    Mdm                 = []      #mass dark matter\n",
    "    Ms                  = []      #mass stars\n",
    "    Mgas                = []      #mass gas\n",
    "    Rmax                = []      #max R?\n",
    "\n",
    "    FOFCOP              = []\n",
    "    R200                = []\n",
    "    M200                = []\n",
    "    FirstSubID          = []  \n",
    "    Contamination_count = []      #number of subhalos that are lower res\n",
    "                                #we won't need this as we will avoid contamination particles by selecting subhalos inside a 2Mpc sphere.\n",
    "\n",
    "\n",
    "\n",
    "    with h5py.File(gpath+gfile+'.0.hdf5', 'r') as f:\n",
    "        a = f['Header'].attrs['Time']                       #scale factor\n",
    "        h = f['Header'].attrs['HubbleParam'] \n",
    "            \n",
    "    n=0\n",
    "    while os.path.exists(gpath+gfile+'.'+str(n)+'.hdf5'):\n",
    "        with h5py.File(gpath+gfile+'.'+str(n)+'.hdf5', 'r') as f:\n",
    "            try:\n",
    "                fofs.extend(f['Subhalo/GroupNumber'][()])\n",
    "                subs.extend(f['Subhalo/SubGroupNumber'][()])\n",
    "                cops.extend(f['Subhalo/CentreOfPotential'][()]*a/h)   \n",
    "                Vmax.extend(f['Subhalo/Vmax'][()])\n",
    "                Rmax.extend(f['Subhalo/VmaxRadius'][()])   \n",
    "                Mgas.extend(f['Subhalo/MassType'][:,0]*1e10/h)\n",
    "                Mdm.extend(f['Subhalo/MassType'][:,1]*1e10/h)\n",
    "                Ms.extend(f['Subhalo/MassType'][:,4]*1e10/h)\n",
    "\n",
    "                FOFCOP.extend(f['FOF/GroupCentreOfPotential']*a/h)\n",
    "                R200.extend(f['FOF/Group_R_Crit200'][()]*a/h)\n",
    "                M200.extend(f['FOF/Group_M_Crit200'][()]*1e10/h)\n",
    "                FirstSubID.extend(f['FOF/FirstSubhaloID'][()])\n",
    "                Contamination_count.extend( f['FOF/ContaminationCount'][()])\n",
    "            except:\n",
    "                pass\n",
    "            n+=1\n",
    "\n",
    "\n",
    "\n",
    "    print('Sucessfully read {} files.'.format(n))\n",
    "\n",
    "\n",
    "    # It  is very useful to work with \"numpy arrays\" (mathematical calculations are allowed). So I transform the python lists to  numpy arrays. (This I will do all the time as you will see below)\n",
    "    # these arrays have length = to the number of subhalos\n",
    "    fofs = np.array(fofs)   \n",
    "    subs = np.array(subs)\n",
    "    cops = np.array(cops)\n",
    "    Vmax = np.array(Vmax)\n",
    "    Mdm  = np.array(Mdm)\n",
    "    Ms   = np.array(Ms)\n",
    "    Mgas = np.array(Mgas)\n",
    "    Rmax = np.array(Rmax)  \n",
    "\n",
    "    subs_indxs = np.arange(0,len(subs))\n",
    "\n",
    "    # these arrays have length = to the number of fofs\n",
    "    FOFCOP     = np.array(FOFCOP)\n",
    "    R200       = np.array(R200)\n",
    "    M200       = np.array(M200)\n",
    "    FirstSubID = np.array(FirstSubID)\n",
    "    Contamination_count= np.array(Contamination_count)\n",
    "\n",
    "\n",
    "    #fig,ax = plt.subplots()\n",
    "    #ax.loglog(Mgas, Ms, 'ko')\n",
    "    #ax.set_xlabel('Mgas')\n",
    "    #ax.set_ylabel('Mstar')\n",
    "\n",
    "    ### In principle, we only want to keep subhalos with virial masses M200>1e10 Msun (less is poorer resolution)\n",
    "    fofIDs= np.where( (Contamination_count==0) )   # (no contamination particles)                      ### WARNING\n",
    "    #Since FOF groups are ordered by mass, starting at fofID=1, this numpy.where array is giving me (FofID-1). \n",
    "\n",
    "    FOFCOP = FOFCOP[fofIDs]\n",
    "    M200   = M200[fofIDs] \n",
    "    R200   = R200[fofIDs]\n",
    "\n",
    "\n",
    "    # the real fofID number is the numpy.where array above +1\n",
    "    fofIDs_real = fofIDs[0] + 1\n",
    "    which       = np.isin( fofs, fofIDs_real) \n",
    "    #It filters out the fofs we are interested in  (i.e., fofIDs_real) from the total fofs list (i.e., fofs).\n",
    "\n",
    "    fofs = fofs[which]  \n",
    "    subs = subs[which]\n",
    "    cops = cops[which]\n",
    "    Vmax = Vmax[which]\n",
    "    Mdm  = Mdm[which]\n",
    "    Ms   = Ms[which]\n",
    "    Mgas = Mgas[which]\n",
    "    Rmax = Rmax[which]\n",
    "\n",
    "    subs_indxs = subs_indxs[which]\n",
    "\n",
    "    which = np.isin(fofIDs_real, fofs)\n",
    "    M200  = M200[which]\n",
    "    R200  = R200[which]\n",
    "\n",
    "    dict_out['fofs']       = fofs\n",
    "    dict_out['subs']       = subs\n",
    "    dict_out['cops']       = cops\n",
    "    # dict_out['Vmax']       = Vmax\n",
    "    dict_out['Mdm']        = Mdm\n",
    "    dict_out['Mstar']      = Ms\n",
    "    dict_out['Mgas']       = Mgas\n",
    "    # dict_out['M200']       = M200\n",
    "    # dict_out['R200']       = R200\n",
    "    dict_out['subf_id'] = subs_indxs\n",
    "\n",
    "    # dict_out['ctr_midp']   = ctr_midpoint\n",
    "\n",
    "    return dict_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "726c87ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sucessfully read 192 files.\n",
      "[ 9.104561 17.614407 84.18322 ]\n",
      "(6467, 9)\n"
     ]
    }
   ],
   "source": [
    "output_arr = np.empty((0, 9))\n",
    "for snap_label, snap_num in zip(snap_label_list, snap_num_list):\n",
    "\n",
    "  datasuffix = snap_label\n",
    "  gpath = datapath+'/groups_'+datasuffix+'/'\n",
    "  gfile = 'eagle_subfind_tab_'+datasuffix\n",
    "\n",
    "  dict_out = read_subgroup_file(gpath, gfile)\n",
    "\n",
    "  snap_arr = np.zeros(len(dict_out['fofs'])) + snap_num\n",
    "\n",
    "  output_arr_temp = np.concatenate( (dict_out['fofs'].reshape(-1,1),\n",
    "                                      dict_out['subs'].reshape(-1,1),\n",
    "                                      snap_arr.reshape(-1,1),\n",
    "                                        dict_out['subf_id'].reshape(-1,1),\n",
    "                                          dict_out['Mdm'].reshape(-1,1),\n",
    "                                          dict_out['Mgas'].reshape(-1,1),\n",
    "                                          dict_out['Mstar'].reshape(-1,1),\n",
    "                                            dict_out['cops'].reshape(-1,3)), axis=1)\n",
    "  print(output_arr_temp.shape)\n",
    "  output_arr = np.concatenate((output_arr, output_arr_temp), axis=0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9f9d13ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[127. 127. 127. ... 127. 127. 127.]\n"
     ]
    }
   ],
   "source": [
    "print(output_arr[:,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "10fcc14e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(data=output_arr, columns=['fofs', 'subs', 'snpnr', 'subf_id', 'Mdm', 'Mgas', 'Mstar', 'cops_x', 'cops_y', 'cops_z'])#, dtype=[int, float, float, float, float, float])\n",
    "df['fofs']  = df['fofs'].astype('int')\n",
    "df['subs']  = df['subs'].astype('int')\n",
    "df['snpnr'] = df['snpnr'].astype('int')\n",
    "df['subf_id'] = df['subf_id'].astype('int')\n",
    "\n",
    "df.to_csv(path_or_buf='SUBGROUP_TABLE_MR.csv', sep=',', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "f8fda810",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9bc8c577",
   "metadata": {},
   "source": [
    "# Create values for TABLE PARTICLEDM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "0a9e786f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_particles(group_file, particle_file, ptype_flag): \n",
    "    '''\n",
    "    loads in groupnumber, subgroupnumber, particleid for particle data file\n",
    "    '''\n",
    "    \n",
    "    with h5py.File(group_file+'.0.hdf5', 'r') as f:\n",
    "        a = f['Header'].attrs['Time']                       #scale factor\n",
    "        h = f['Header'].attrs['HubbleParam'] \n",
    "\n",
    "    pid = np.empty(0, dtype=int)\n",
    "    gnr = np.empty(0, dtype=int)\n",
    "    snr = np.empty(0, dtype=int)\n",
    "        \n",
    "    #print(particle_file+'.'+str(0)+'.hdf5')\n",
    "\n",
    "    n=0\n",
    "    while os.path.exists(particle_file+'.'+str(n)+'.hdf5'):\n",
    "        with h5py.File(particle_file+'.'+str(n)+'.hdf5', 'r') as f:\n",
    "            \n",
    "            \n",
    "            try:\n",
    "                pid = np.append(pid, f['PartType{}/ParticleIDs'.format(ptype_flag)][()], axis=0)\n",
    "                gnr = np.append(gnr, f['PartType{}/GroupNumber'.format(ptype_flag)][()], axis=0)\n",
    "                snr = np.append(snr, f['PartType{}/SubGroupNumber'.format(ptype_flag)][()], axis=0)\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "            n+=1\n",
    "\n",
    "    \n",
    "    print('Sucessfully read {} Particle Data files.'.format(n))\n",
    "\n",
    "    return gnr, snr, pid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "606f0489",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_snapshot(group_file, snapshot_file, ptype_flag): \n",
    "    '''\n",
    "    loads in groupnumber, subgroupnumber, particleid for particle data file\n",
    "    '''\n",
    "\n",
    "    with h5py.File(group_file+'.0.hdf5', 'r') as f:\n",
    "        a = f['Header'].attrs['Time']                       #scale factor\n",
    "        h = f['Header'].attrs['HubbleParam'] \n",
    "\n",
    "    pid = np.empty(0, dtype=int)\n",
    "    xyz = np.empty((0,3), dtype=float)\n",
    "    hsml = np.empty(0, dtype=float)\n",
    "    mass = np.empty(0, dtype=float)\n",
    "    \n",
    "    n=0\n",
    "    while os.path.exists(snapshot_file+'.'+str(n)+'.hdf5'):\n",
    "        with h5py.File(snapshot_file+'.'+str(n)+'.hdf5', 'r') as f:\n",
    "\n",
    "            if ptype_flag == 4:  #'''Not all snapshots will have stars.'''\n",
    "                \n",
    "                try:\n",
    "                    pid = np.append(pid, f['PartType{}/ParticleIDs'.format(ptype_flag)][()], axis=0)\n",
    "                    xyz = np.append(xyz, f['PartType{}/Coordinates'.format(ptype_flag)][()]*a/h, axis=0) #Mpc\n",
    "\n",
    "                    if ptype_flag == 0 or ptype_flag == 4:\n",
    "                        mass = np.append(mass, f['PartType{}/Masses'.format(ptype_flag)][()]*1.e10/h, axis=0)\n",
    "\n",
    "                    if ptype_flag == 0: # gas\n",
    "                        hsml = np.append(hsml, f['PartType0/SmoothingLength'][()], axis=0)\n",
    "                except: \n",
    "                    pass\n",
    "            \n",
    "            else:\n",
    "                pid = np.append(pid, f['PartType{}/ParticleIDs'.format(ptype_flag)][()], axis=0)\n",
    "                xyz = np.append(xyz, f['PartType{}/Coordinates'.format(ptype_flag)][()]*a/h, axis=0) #Mpc\n",
    "\n",
    "                if ptype_flag == 0 or ptype_flag == 4:\n",
    "                    mass = np.append(mass, f['PartType{}/Masses'.format(ptype_flag)][()]*1.e10/h, axis=0)\n",
    "\n",
    "                if ptype_flag == 0: # gas\n",
    "                    hsml = np.append(hsml, f['PartType0/SmoothingLength'][()], axis=0)\n",
    "\n",
    "            n+=1\n",
    "    print('Sucessfully read {} Particle Data files.'.format(n))\n",
    "\n",
    "    print(len(pid))\n",
    "    if ptype_flag == 0:\n",
    "        return pid, xyz, mass, hsml\n",
    "    elif ptype_flag == 4:\n",
    "        return pid, xyz, mass\n",
    "    else:\n",
    "        return pid, xyz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "c26653b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sucessfully read 16 Particle Data files.\n",
      "Sucessfully read 16 Particle Data files.\n",
      "1759284\n",
      "Sucessfully read 16 Particle Data files.\n",
      "Sucessfully read 16 Particle Data files.\n",
      "1759284\n",
      "Sucessfully read 16 Particle Data files.\n",
      "Sucessfully read 16 Particle Data files.\n",
      "1759284\n",
      "Sucessfully read 16 Particle Data files.\n",
      "Sucessfully read 16 Particle Data files.\n",
      "1759284\n",
      "Sucessfully read 16 Particle Data files.\n",
      "Sucessfully read 16 Particle Data files.\n",
      "1759284\n",
      "Sucessfully read 16 Particle Data files.\n",
      "Sucessfully read 16 Particle Data files.\n",
      "1759284\n",
      "Sucessfully read 16 Particle Data files.\n",
      "Sucessfully read 16 Particle Data files.\n",
      "1759284\n",
      "Sucessfully read 16 Particle Data files.\n",
      "Sucessfully read 16 Particle Data files.\n",
      "1759284\n",
      "Sucessfully read 16 Particle Data files.\n",
      "Sucessfully read 16 Particle Data files.\n",
      "1759284\n",
      "Sucessfully read 16 Particle Data files.\n",
      "Sucessfully read 16 Particle Data files.\n",
      "1759284\n",
      "Sucessfully read 16 Particle Data files.\n",
      "Sucessfully read 16 Particle Data files.\n",
      "1759284\n",
      "Sucessfully read 16 Particle Data files.\n",
      "Sucessfully read 16 Particle Data files.\n",
      "1759284\n",
      "Sucessfully read 16 Particle Data files.\n",
      "Sucessfully read 16 Particle Data files.\n",
      "1759284\n",
      "Sucessfully read 16 Particle Data files.\n",
      "Sucessfully read 16 Particle Data files.\n",
      "1759284\n",
      "Sucessfully read 16 Particle Data files.\n",
      "Sucessfully read 16 Particle Data files.\n",
      "1759284\n",
      "Sucessfully read 16 Particle Data files.\n",
      "Sucessfully read 16 Particle Data files.\n",
      "1759284\n",
      "Sucessfully read 16 Particle Data files.\n",
      "Sucessfully read 16 Particle Data files.\n",
      "1759284\n",
      "Sucessfully read 16 Particle Data files.\n",
      "Sucessfully read 16 Particle Data files.\n",
      "1759284\n",
      "Sucessfully read 16 Particle Data files.\n",
      "Sucessfully read 16 Particle Data files.\n",
      "1759284\n",
      "Sucessfully read 16 Particle Data files.\n",
      "Sucessfully read 16 Particle Data files.\n",
      "1759284\n",
      "Sucessfully read 16 Particle Data files.\n",
      "Sucessfully read 16 Particle Data files.\n",
      "1759284\n",
      "Sucessfully read 16 Particle Data files.\n",
      "Sucessfully read 16 Particle Data files.\n",
      "1759284\n",
      "Sucessfully read 16 Particle Data files.\n",
      "Sucessfully read 16 Particle Data files.\n",
      "1759284\n",
      "Sucessfully read 16 Particle Data files.\n",
      "Sucessfully read 16 Particle Data files.\n",
      "1759284\n",
      "Sucessfully read 16 Particle Data files.\n",
      "Sucessfully read 16 Particle Data files.\n",
      "1759284\n",
      "Sucessfully read 16 Particle Data files.\n",
      "Sucessfully read 16 Particle Data files.\n",
      "1759284\n",
      "Sucessfully read 16 Particle Data files.\n",
      "Sucessfully read 16 Particle Data files.\n",
      "1759284\n",
      "Sucessfully read 16 Particle Data files.\n",
      "Sucessfully read 16 Particle Data files.\n",
      "1759284\n",
      "Sucessfully read 16 Particle Data files.\n",
      "Sucessfully read 16 Particle Data files.\n",
      "1759284\n",
      "Sucessfully read 16 Particle Data files.\n",
      "Sucessfully read 16 Particle Data files.\n",
      "1759284\n",
      "Sucessfully read 16 Particle Data files.\n",
      "Sucessfully read 16 Particle Data files.\n",
      "1759284\n",
      "Sucessfully read 16 Particle Data files.\n",
      "Sucessfully read 16 Particle Data files.\n",
      "1759284\n",
      "Sucessfully read 16 Particle Data files.\n",
      "Sucessfully read 16 Particle Data files.\n",
      "1759284\n",
      "Sucessfully read 16 Particle Data files.\n",
      "Sucessfully read 16 Particle Data files.\n",
      "1759284\n",
      "Sucessfully read 16 Particle Data files.\n",
      "Sucessfully read 16 Particle Data files.\n",
      "1759284\n",
      "Sucessfully read 16 Particle Data files.\n",
      "Sucessfully read 16 Particle Data files.\n",
      "1759284\n",
      "Sucessfully read 16 Particle Data files.\n",
      "Sucessfully read 16 Particle Data files.\n",
      "1759284\n",
      "Sucessfully read 16 Particle Data files.\n",
      "Sucessfully read 16 Particle Data files.\n",
      "1759284\n",
      "Sucessfully read 16 Particle Data files.\n",
      "Sucessfully read 16 Particle Data files.\n",
      "1759284\n",
      "Sucessfully read 16 Particle Data files.\n",
      "Sucessfully read 16 Particle Data files.\n",
      "1759284\n",
      "Sucessfully read 16 Particle Data files.\n",
      "Sucessfully read 16 Particle Data files.\n",
      "1759284\n",
      "Sucessfully read 16 Particle Data files.\n",
      "Sucessfully read 16 Particle Data files.\n",
      "1759284\n",
      "Sucessfully read 16 Particle Data files.\n",
      "Sucessfully read 16 Particle Data files.\n",
      "1759284\n",
      "Sucessfully read 16 Particle Data files.\n",
      "Sucessfully read 16 Particle Data files.\n",
      "1759284\n",
      "Sucessfully read 16 Particle Data files.\n",
      "Sucessfully read 16 Particle Data files.\n",
      "1759284\n",
      "Sucessfully read 16 Particle Data files.\n",
      "Sucessfully read 16 Particle Data files.\n",
      "1759284\n",
      "Sucessfully read 16 Particle Data files.\n",
      "Sucessfully read 16 Particle Data files.\n",
      "1759284\n",
      "Sucessfully read 16 Particle Data files.\n",
      "Sucessfully read 16 Particle Data files.\n",
      "1759284\n",
      "Sucessfully read 16 Particle Data files.\n",
      "Sucessfully read 16 Particle Data files.\n",
      "1759284\n",
      "Sucessfully read 16 Particle Data files.\n",
      "Sucessfully read 16 Particle Data files.\n",
      "1759284\n",
      "Sucessfully read 16 Particle Data files.\n",
      "Sucessfully read 16 Particle Data files.\n",
      "1759284\n",
      "Sucessfully read 16 Particle Data files.\n",
      "Sucessfully read 16 Particle Data files.\n",
      "1759284\n",
      "Sucessfully read 16 Particle Data files.\n",
      "Sucessfully read 16 Particle Data files.\n",
      "1759284\n",
      "Sucessfully read 16 Particle Data files.\n",
      "Sucessfully read 16 Particle Data files.\n",
      "1759284\n",
      "Sucessfully read 16 Particle Data files.\n",
      "Sucessfully read 16 Particle Data files.\n",
      "1759284\n",
      "Sucessfully read 16 Particle Data files.\n",
      "Sucessfully read 16 Particle Data files.\n",
      "1759284\n",
      "Sucessfully read 16 Particle Data files.\n",
      "Sucessfully read 16 Particle Data files.\n",
      "1759284\n",
      "Sucessfully read 16 Particle Data files.\n",
      "Sucessfully read 16 Particle Data files.\n",
      "1759284\n",
      "Sucessfully read 16 Particle Data files.\n",
      "Sucessfully read 16 Particle Data files.\n",
      "1759284\n",
      "Sucessfully read 16 Particle Data files.\n",
      "Sucessfully read 16 Particle Data files.\n",
      "1759284\n",
      "Sucessfully read 16 Particle Data files.\n",
      "Sucessfully read 16 Particle Data files.\n",
      "1759284\n",
      "Sucessfully read 16 Particle Data files.\n",
      "Sucessfully read 16 Particle Data files.\n",
      "1759284\n",
      "Sucessfully read 16 Particle Data files.\n",
      "Sucessfully read 16 Particle Data files.\n",
      "1759284\n",
      "Sucessfully read 16 Particle Data files.\n",
      "Sucessfully read 16 Particle Data files.\n",
      "1759284\n",
      "Sucessfully read 16 Particle Data files.\n",
      "Sucessfully read 16 Particle Data files.\n",
      "1759284\n",
      "Sucessfully read 16 Particle Data files.\n",
      "Sucessfully read 16 Particle Data files.\n",
      "1759284\n",
      "Sucessfully read 16 Particle Data files.\n",
      "Sucessfully read 16 Particle Data files.\n",
      "1759284\n",
      "Sucessfully read 16 Particle Data files.\n",
      "Sucessfully read 16 Particle Data files.\n",
      "1759284\n",
      "Sucessfully read 16 Particle Data files.\n",
      "Sucessfully read 16 Particle Data files.\n",
      "1759284\n",
      "Sucessfully read 16 Particle Data files.\n",
      "Sucessfully read 16 Particle Data files.\n",
      "1759284\n",
      "Sucessfully read 16 Particle Data files.\n",
      "Sucessfully read 16 Particle Data files.\n",
      "1759284\n",
      "Sucessfully read 16 Particle Data files.\n",
      "Sucessfully read 16 Particle Data files.\n",
      "1759284\n",
      "Sucessfully read 16 Particle Data files.\n",
      "Sucessfully read 16 Particle Data files.\n",
      "1759284\n",
      "Sucessfully read 16 Particle Data files.\n",
      "Sucessfully read 16 Particle Data files.\n",
      "1759284\n",
      "Sucessfully read 16 Particle Data files.\n",
      "Sucessfully read 16 Particle Data files.\n",
      "1759284\n",
      "Sucessfully read 16 Particle Data files.\n",
      "Sucessfully read 16 Particle Data files.\n",
      "1759284\n",
      "Sucessfully read 16 Particle Data files.\n",
      "Sucessfully read 16 Particle Data files.\n",
      "1759284\n",
      "Sucessfully read 16 Particle Data files.\n",
      "Sucessfully read 16 Particle Data files.\n",
      "1759284\n",
      "Sucessfully read 16 Particle Data files.\n",
      "Sucessfully read 16 Particle Data files.\n",
      "1759284\n",
      "Sucessfully read 16 Particle Data files.\n",
      "Sucessfully read 16 Particle Data files.\n",
      "1759284\n",
      "Sucessfully read 16 Particle Data files.\n",
      "Sucessfully read 16 Particle Data files.\n",
      "1759284\n",
      "Sucessfully read 16 Particle Data files.\n",
      "Sucessfully read 16 Particle Data files.\n",
      "1759284\n",
      "Sucessfully read 16 Particle Data files.\n",
      "Sucessfully read 16 Particle Data files.\n",
      "1759284\n",
      "Sucessfully read 16 Particle Data files.\n",
      "Sucessfully read 16 Particle Data files.\n",
      "1759284\n",
      "Sucessfully read 16 Particle Data files.\n",
      "Sucessfully read 16 Particle Data files.\n",
      "1759284\n",
      "Sucessfully read 16 Particle Data files.\n",
      "Sucessfully read 16 Particle Data files.\n",
      "1759284\n",
      "Sucessfully read 16 Particle Data files.\n",
      "Sucessfully read 16 Particle Data files.\n",
      "1759284\n",
      "Sucessfully read 16 Particle Data files.\n",
      "Sucessfully read 16 Particle Data files.\n",
      "1759284\n",
      "Sucessfully read 16 Particle Data files.\n",
      "Sucessfully read 16 Particle Data files.\n",
      "1759284\n",
      "Sucessfully read 16 Particle Data files.\n",
      "Sucessfully read 16 Particle Data files.\n",
      "1759284\n",
      "Sucessfully read 16 Particle Data files.\n",
      "Sucessfully read 16 Particle Data files.\n",
      "1759284\n",
      "Sucessfully read 16 Particle Data files.\n",
      "Sucessfully read 16 Particle Data files.\n",
      "1759284\n",
      "Sucessfully read 16 Particle Data files.\n",
      "Sucessfully read 16 Particle Data files.\n",
      "1759284\n",
      "Sucessfully read 16 Particle Data files.\n",
      "Sucessfully read 16 Particle Data files.\n",
      "1759284\n",
      "Sucessfully read 16 Particle Data files.\n",
      "Sucessfully read 16 Particle Data files.\n",
      "1759284\n",
      "Sucessfully read 16 Particle Data files.\n",
      "Sucessfully read 16 Particle Data files.\n",
      "1759284\n",
      "Sucessfully read 16 Particle Data files.\n",
      "Sucessfully read 16 Particle Data files.\n",
      "1759284\n",
      "Sucessfully read 16 Particle Data files.\n",
      "Sucessfully read 16 Particle Data files.\n",
      "1759284\n",
      "Sucessfully read 16 Particle Data files.\n",
      "Sucessfully read 16 Particle Data files.\n",
      "1759284\n",
      "Sucessfully read 16 Particle Data files.\n",
      "Sucessfully read 16 Particle Data files.\n",
      "1759284\n",
      "Sucessfully read 16 Particle Data files.\n",
      "Sucessfully read 16 Particle Data files.\n",
      "1759284\n",
      "Sucessfully read 16 Particle Data files.\n",
      "Sucessfully read 16 Particle Data files.\n",
      "1759284\n",
      "Sucessfully read 16 Particle Data files.\n",
      "Sucessfully read 16 Particle Data files.\n",
      "1759284\n",
      "Sucessfully read 16 Particle Data files.\n",
      "Sucessfully read 16 Particle Data files.\n",
      "1759284\n",
      "Sucessfully read 16 Particle Data files.\n",
      "Sucessfully read 16 Particle Data files.\n",
      "1759284\n",
      "Sucessfully read 16 Particle Data files.\n",
      "Sucessfully read 16 Particle Data files.\n",
      "1759284\n",
      "Sucessfully read 16 Particle Data files.\n",
      "Sucessfully read 16 Particle Data files.\n",
      "1759284\n",
      "Sucessfully read 16 Particle Data files.\n",
      "Sucessfully read 16 Particle Data files.\n",
      "1759284\n",
      "Sucessfully read 16 Particle Data files.\n",
      "Sucessfully read 16 Particle Data files.\n",
      "1759284\n",
      "Sucessfully read 16 Particle Data files.\n",
      "Sucessfully read 16 Particle Data files.\n",
      "1759284\n",
      "Sucessfully read 16 Particle Data files.\n",
      "Sucessfully read 16 Particle Data files.\n",
      "1759284\n",
      "Sucessfully read 16 Particle Data files.\n",
      "Sucessfully read 16 Particle Data files.\n",
      "1759284\n",
      "Sucessfully read 16 Particle Data files.\n",
      "Sucessfully read 16 Particle Data files.\n",
      "1759284\n",
      "Sucessfully read 16 Particle Data files.\n",
      "Sucessfully read 16 Particle Data files.\n",
      "1759284\n",
      "Sucessfully read 16 Particle Data files.\n",
      "Sucessfully read 16 Particle Data files.\n",
      "1759284\n",
      "Sucessfully read 16 Particle Data files.\n",
      "Sucessfully read 16 Particle Data files.\n",
      "1759284\n",
      "Sucessfully read 16 Particle Data files.\n",
      "Sucessfully read 16 Particle Data files.\n",
      "1759284\n",
      "Sucessfully read 16 Particle Data files.\n",
      "Sucessfully read 16 Particle Data files.\n",
      "1759284\n",
      "Sucessfully read 16 Particle Data files.\n",
      "Sucessfully read 16 Particle Data files.\n",
      "1759284\n",
      "Sucessfully read 16 Particle Data files.\n",
      "Sucessfully read 16 Particle Data files.\n",
      "1759284\n",
      "Sucessfully read 16 Particle Data files.\n",
      "Sucessfully read 16 Particle Data files.\n",
      "1759284\n",
      "Sucessfully read 16 Particle Data files.\n",
      "Sucessfully read 16 Particle Data files.\n",
      "1759284\n",
      "Sucessfully read 16 Particle Data files.\n",
      "Sucessfully read 16 Particle Data files.\n",
      "1759284\n",
      "Sucessfully read 16 Particle Data files.\n",
      "Sucessfully read 16 Particle Data files.\n",
      "1759284\n",
      "Sucessfully read 16 Particle Data files.\n",
      "Sucessfully read 16 Particle Data files.\n",
      "1759284\n",
      "Sucessfully read 16 Particle Data files.\n",
      "Sucessfully read 16 Particle Data files.\n",
      "1759284\n",
      "Sucessfully read 16 Particle Data files.\n",
      "Sucessfully read 16 Particle Data files.\n",
      "1759284\n",
      "Sucessfully read 16 Particle Data files.\n",
      "Sucessfully read 16 Particle Data files.\n",
      "1759284\n"
     ]
    }
   ],
   "source": [
    "datapath   = '../data/V1_LR_fix'\n",
    "ptype_flag = 1\n",
    "\n",
    "for snap_label, snap_num in zip(snap_label_list, snap_num_list):\n",
    "\n",
    "    datasuffix = snap_label\n",
    "\n",
    "    #group data\n",
    "    gpath = datapath+'/groups_'+datasuffix+'/'\n",
    "    gfile = 'eagle_subfind_tab_'+datasuffix\n",
    "\n",
    "    #particle data\n",
    "    ppath = datapath+'/particledata_'+datasuffix+'/'\n",
    "    pfile = 'eagle_subfind_particles_'+datasuffix\n",
    "\n",
    "    #snapshot data\n",
    "    spath = datapath+'/snapshot_'+datasuffix+'/'\n",
    "    sfile = 'snap_'+datasuffix\n",
    "\n",
    "\n",
    "    ''' Load in PARTICLE DATA'''\n",
    "\n",
    "    particles_out = load_particles(gpath+gfile, ppath+pfile, ptype_flag)\n",
    "\n",
    "    gnr  = particles_out[0]\n",
    "    snr  = particles_out[1]\n",
    "    pid  = particles_out[2]\n",
    "\n",
    "    \n",
    "    ''' Load in SNAPSHOT DATA'''\n",
    "\n",
    "    snapshot_out = load_snapshot(gpath+gfile, spath+sfile, ptype_flag)\n",
    "\n",
    "    snap_pid = snapshot_out[0]\n",
    "    snap_xyz = snapshot_out[1]\n",
    "\n",
    "    if ptype_flag == 0:\n",
    "        snap_mass = snapshot_out[2]\n",
    "        snap_hsml = snapshot_out[3]\n",
    "    elif ptype_flag == 4:\n",
    "        snap_mass = snapshot_out[2]\n",
    "\n",
    "\n",
    "    ''' Clean Up FOFID and SUBID which are INVALID'''\n",
    "\n",
    "    snap_gnr = np.zeros(len(snap_pid), dtype=int) -999\n",
    "    snap_snr = np.zeros(len(snap_pid), dtype=int) -999\n",
    "\n",
    "    snap_gnr[np.isin(snap_pid, pid)] = gnr\n",
    "    snap_snr[np.isin(snap_pid, pid)] = snr\n",
    "\n",
    "    snap_gnr[snap_gnr< 0] = -999\n",
    "    snap_snr[snap_snr< 0] = -999\n",
    "\n",
    "    ''' Prepare OUTPUT ARRAY'''\n",
    "\n",
    "    if ptype_flag == 0:\n",
    "        output_arr = np.concatenate((snap_gnr.reshape(-1,1),\n",
    "                                    snap_snr.reshape(-1,1),\n",
    "                                    snap_pid.reshape(-1,1),\n",
    "                                    snap_xyz.reshape(-1,3),\n",
    "                                    snap_mass.reshape(-1,1),\n",
    "                                    snap_hsml.reshape(-1,1)), axis=1)\n",
    "\n",
    "    elif ptype_flag == 4:\n",
    "        output_arr = np.concatenate((snap_gnr.reshape(-1,1),\n",
    "                                    snap_snr.reshape(-1,1),\n",
    "                                    snap_pid.reshape(-1,1),\n",
    "                                    snap_xyz.reshape(-1,3),\n",
    "                                    snap_mass.reshape(-1,1)), axis=1)\n",
    "\n",
    "    else:\n",
    "        output_arr = np.concatenate((snap_gnr.reshape(-1,1),\n",
    "                                    snap_snr.reshape(-1,1),\n",
    "                                    snap_pid.reshape(-1,1),\n",
    "                                    snap_xyz.reshape(-1,3)), axis=1)\n",
    "        \n",
    "    ''' Setup PANDAS DF for CSV Export'''\n",
    "\n",
    "    if ptype_flag == 0:\n",
    "        df = pd.DataFrame(data=output_arr, columns=['fofs', 'subs', 'pids', 'coord_x', 'coord_y', 'coord_z', 'mass', 'hsml'])\n",
    "    elif ptype_flag == 4:\n",
    "        df = pd.DataFrame(data=output_arr, columns=['fofs', 'subs', 'pids', 'coord_x', 'coord_y', 'coord_z', 'mass'])\n",
    "    else:\n",
    "        df = pd.DataFrame(data=output_arr, columns=['fofs', 'subs', 'pids', 'coord_x', 'coord_y', 'coord_z'])\n",
    "\n",
    "    df['fofs'] = df['fofs'].astype('int')\n",
    "    df['subs'] = df['subs'].astype('int')\n",
    "    df['pids'] = df['pids'].astype('int')\n",
    "\n",
    "\n",
    "    df.sort_values(by=['fofs','subs'], inplace=True)\n",
    "\n",
    "\n",
    "\n",
    "    if ptype_flag == 0:\n",
    "        val = 'GAS'\n",
    "\n",
    "    elif ptype_flag == 1:\n",
    "        val = 'DM'\n",
    "\n",
    "    elif ptype_flag == 4:\n",
    "        val = 'STAR'\n",
    "        \n",
    "    else:\n",
    "        val = 'WRONG'\n",
    "\n",
    "    \n",
    "    df.to_csv(path_or_buf='./SNAPSHOTS_LR_{}/SNAPSHOT_{}_V1_LR_{}.csv'.format(val, val, snap_num), sep=',', index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ba8aa5d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f68b04fb",
   "metadata": {},
   "source": [
    "# Create values for MERGERTREE TABLE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1b7fe602",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_mergertree_file(mpath, mfile):\n",
    "    \n",
    "    desc_id      = []\n",
    "    node_id      = []\n",
    "    id_main_prog = []\n",
    "    subf_id      = []\n",
    "    snpnr_all    = []\n",
    "    \n",
    "    print(mpath + mfile + '/' + 'tree_127.'+'0' + '.hdf5')\n",
    "\n",
    "    n =\t0\n",
    "    while os.path.exists( mpath + mfile + '/' + 'tree_127.'+str(n) + '.hdf5'):\n",
    "        filein = mpath + mfile + '/' + 'tree_127.'+str(n) + '.hdf5'\n",
    "        \n",
    "        with h5py.File(filein, 'r') as f:\n",
    "            desc_id.extend( f['haloTrees/descendantIndex'][()])\n",
    "            node_id.extend( f['haloTrees/nodeIndex'][()])\n",
    "            id_main_prog.extend( f['/haloTrees/mainProgenitorIndex'][()])\n",
    "            subf_id.extend( f['/haloTrees/positionInCatalogue'][()])\n",
    "            snpnr_all.extend( f['/haloTrees/snapshotNumber'][()]) \n",
    "\n",
    "            n+=1\n",
    "\n",
    "    print('Sucessfully read {} files.'.format(n))\n",
    "\n",
    "\n",
    "    desc_id = np.array(desc_id)\n",
    "    node_id = np.array(node_id)\n",
    "    subf_id = np.array(subf_id)\n",
    "    snpnr_all = np.array(snpnr_all)\n",
    "    id_main_prog = np.array(id_main_prog)\n",
    "\n",
    "\n",
    "    dict_out = {}\n",
    "\n",
    "    dict_out['desc_id']      = desc_id\n",
    "    dict_out['node_id']      = node_id\n",
    "    dict_out['subf_id']      = subf_id\n",
    "    dict_out['snpnr_all']    = snpnr_all\n",
    "    dict_out['id_main_prog'] = id_main_prog\n",
    "    \n",
    "    return dict_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a8a90da7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../data/V1_LR_fix/merger_tree/tree_127.0.hdf5\n",
      "Sucessfully read 1 files.\n"
     ]
    }
   ],
   "source": [
    "datapath   = '../data/V1_LR_fix'\n",
    "mpath = datapath + '/'\n",
    "mfile = 'merger_tree'\n",
    "\n",
    "dict_out = read_mergertree_file(mpath, mfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2b26b0e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_arr = np.concatenate( (dict_out['desc_id'].reshape(-1,1),\n",
    "                                dict_out['node_id'].reshape(-1,1),\n",
    "                                  dict_out['id_main_prog'].reshape(-1,1),\n",
    "                                    dict_out['subf_id'].reshape(-1,1),\n",
    "                                      dict_out['snpnr_all'].reshape(-1,1)), axis=1, dtype=int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "43ff3fa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(data=output_arr, columns=['desc_id', 'node_id', 'id_main_prog', 'subf_id', 'snapnr'])#, dtype=[int, float, float, float, float, float])\n",
    "df.to_csv(path_or_buf='MergerTree_V1_LR.csv', sep=',', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
